{"cells":[{"cell_type":"code","source":["from pyspark import SparkContext\n\nwords = sc.parallelize (\n   [\"scala\", \n   \"java\", \n   \"hadoop\", \n   \"spark\", \n   \"akka\",\n   \"spark vs hadoop\", \n   \"pyspark\",\n   \"pyspark and spark\"]\n)\n\n#counts = words.count()\n#print (\"Number of elements in RDD -> %i\" % (counts))\n\ncoll = words.collect()\nprint (\"Elements in RDD -> %s\" % (coll))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.RDD&#39;&gt;\nElements in RDD -&gt; [&#39;scala&#39;, &#39;java&#39;, &#39;hadoop&#39;, &#39;spark&#39;, &#39;akka&#39;, &#39;spark vs hadoop&#39;, &#39;pyspark&#39;, &#39;pyspark and spark&#39;]\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["words = sc.parallelize (\n   [\"scala\", \n   \"java\", \n   \"hadoop\", \n   \"spark\", \n   \"akka\",\n   \"1\"]\n)\nprint (\"Join RDD -> %s\" % (words))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Join RDD -&gt; ParallelCollectionRDD[181] at readRDDFromFile at PythonRDD.scala:332\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["def f(x): print(x)\nfore = words.foreach(f) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark import SparkContext\n\nwords = sc.parallelize (\n   [\"scala\", \n   \"java\", \n   \"hadoop\", \n   \"spark\", \n   \"akka\",\n   \"spark vs hadoop\", \n   \"pyspark\",\n   \"pyspark and spark\"]\n)\nwords_filter = words.filter(lambda x: 'spark' in x)\nfiltered = words_filter.collect()\nprint (\"Fitered RDD -> %s\" % (filtered))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fitered RDD -&gt; [&#39;spark&#39;, &#39;spark vs hadoop&#39;, &#39;pyspark&#39;, &#39;pyspark and spark&#39;]\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["words_map = words.map(lambda x: (x))\nmapping = words_map.collect()\nprint (\"Key value pair -> %s\" % (mapping))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Key value pair -&gt; [&#39;scala&#39;, &#39;java&#39;, &#39;hadoop&#39;, &#39;spark&#39;, &#39;akka&#39;, &#39;spark vs hadoop&#39;, &#39;pyspark&#39;, &#39;pyspark and spark&#39;]\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark import SparkContext\nfrom operator import add\n\nnums = sc.parallelize([1, 2, 3, 4, 5])\nadding = nums.reduce(add)\nprint (\"Adding all the elements -> %i\" % (adding))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-55754493499524&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> adding <span class=\"ansi-blue-fg\">=</span> nums<span class=\"ansi-blue-fg\">.</span>reduce<span class=\"ansi-blue-fg\">(</span>add<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> print <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Adding all the elements -&gt; %i&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>adding<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>nums<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>getNumPartition<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;RDD&#39; object has no attribute &#39;rdd&#39;</div>"]}}],"execution_count":6},{"cell_type":"code","source":["x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\ny = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\njoined = x.join(y)\nfinal = joined.collect()\n#converted to list\nprint (\"Join RDD -> %s\" % (final))\n#x.cache() \ncaching = y.persist().is_cached \nprint (\"Words got chached > %s\" % (caching))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Join RDD -&gt; [(&#39;spark&#39;, (1, 2)), (&#39;hadoop&#39;, (4, 5))]\nWords got chached &gt; True\n</div>"]}}],"execution_count":7}],"metadata":{"name":"RDD","notebookId":1023320019972199},"nbformat":4,"nbformat_minor":0}
